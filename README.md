# ğŸš€ PhiloAgent: Production-Grade Intelligent Agent System

This project implements a multi-agent philosophical assistant using LangGraph for orchestration, Groq for low-latency inference, and MongoDB for persistent long-term memory.

---
## ğŸ“ Project Structure
```bash
RAG_Powered_conversationalAI/
â”œâ”€â”€ philoagent/
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ pycache/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ deduplicate_documents.py
â”‚ â”‚ â””â”€â”€ extract.py
â”‚ â”‚
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â””â”€â”€ philoagents/
â”‚ â”‚ â”œâ”€â”€ application/
â”‚ â”‚ â”‚ â”œâ”€â”€ conversation_service/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ pycache/
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”œâ”€â”€ workflow/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ pycache/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ chains.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ edges.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ graph.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ nodes.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ state.py
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ tools.py
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”œâ”€â”€ generate_response.py
â”‚ â”‚ â”‚ â”œâ”€â”€ reset_conversation.py
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”œâ”€â”€ evaluation/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ evaluate.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ generate_dataset.py
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ upload_dataset.py
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”œâ”€â”€ rag/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ retrievers.py
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ splitters.py
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â””â”€â”€ long_term_memory.py
â”‚ â”‚ â”‚
â”‚ â”‚ â”œâ”€â”€ domain/
â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”œâ”€â”€ evaluation.py
â”‚ â”‚ â”‚ â”œâ”€â”€ exceptions.py
â”‚ â”‚ â”‚ â”œâ”€â”€ philosopher.py
â”‚ â”‚ â”‚ â”œâ”€â”€ philosopher_factory.py
â”‚ â”‚ â”‚ â””â”€â”€ prompts.py
â”‚ â”‚ â”‚
â”‚ â”‚ â”œâ”€â”€ infrastructure/
â”‚ â”‚ â”‚ â”œâ”€â”€ mongo/
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ client.py
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ indexes.py
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”œâ”€â”€ api.py
â”‚ â”‚ â”‚ â”œâ”€â”€ api2.py
â”‚ â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”‚ â””â”€â”€ opik_utils.py
â”‚ â”‚ â”‚
â”‚ â”‚ â””â”€â”€ tools/
â”‚ â”‚
â”‚ â”œâ”€â”€ philoagents_ui/
â”‚ â”‚ â”œâ”€â”€ streamlit1.py
â”‚ â”‚ â”œâ”€â”€ streamlit2.py
â”‚ â”‚ â””â”€â”€ streammock.py
â”‚ â”‚
â”‚ â”œâ”€â”€ .dockerignore
â”‚ â”œâ”€â”€ .env
â”‚ â”œâ”€â”€ .env.example
â”‚ â”œâ”€â”€ .python-version
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ docker-compose.yml
â”‚ â”œâ”€â”€ langgraph.json
â”‚ â”œâ”€â”€ pyproject.toml
â”‚ â”œâ”€â”€ uv.lock
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ README.md

```

## Deep Dive: Project Architecture

### 1. Workflow Orchestration (`philoagent/src/application/workflow/`)

The core of the system is a Stateful Finite State Machine (FSM). Unlike simple linear chains, this allows for cycles, retries, and complex decision-making.

- **graph.py**: The central entry point where Nodes and Edges are compiled into a LangGraph.
- **state.py**: Defines the TypedDict that carries the conversation state, including the shared memory buffer and document context.
- **nodes.py & edges.py**: Houses the discrete units of logic (e.g., "Analyze Query", "Retrieve Context") and the conditional routing logic (e.g., "Does the context answer the user's question?").

---

### 2. Advanced RAG Pipeline (`philoagent/src/application/rag/`)

The system uses a robust retrieval strategy specifically optimized for philosophical texts (Stanford Encyclopedia of Philosophy).

- **splitters.py**: Implements semantic or recursive character splitting to maintain philosophical context.
- **retrievers.py**: Contains logic for Hybrid Search or Self-Querying to ensure high-precision retrieval.
- **embeddings.py**: Managed interface for vectorizing queries using modern embedding models.

---

### 3. Domain-Driven Design (`philoagent/src/domain/`)

To ensure the system is "philosopher-agnostic," we use a domain layer to abstract the "Business Logic" of philosophy.

- **philosopher_factory.py**: Uses the Factory Pattern to dynamically load different personas (Socrates, Kant, etc.) with specific behavioral constraints.
- **prompts.py**: A centralized "Prompt Bank" ensuring version control and consistency across different agent nodes.

---

### 4. Infrastructure & Persistence (`philoagent/src/infrastructure/`)

Ensures the system is production-ready with scalable backend services.

- **MongoDB Integration (`mongo/`)**: Handles persistent state and long-term memory, allowing the agent to "remember" users across sessions.
- **FastAPI Layer (`api.py`)**: Exposes the agentic logic as high-performance REST endpoints.
- **High-Speed Inference**: Integrated with Groq via `config.py` to achieve sub-second response times for complex reasoning.

---

### 5. LLMOps & Evaluation (`philoagent/src/application/evaluation/`)

We treat LLM performance as a measurable engineering metric rather than "vibes."

- **Observability**: Integrated with Comet ML Opik (`opik_utils.py`) for full-trace logging, cost monitoring, and prompt versioning.
- **Evaluation Framework (`evaluate.py`)**: Automates RAG evaluation metrics (Faithfulness, Answer Relevance) using a synthetic dataset generated by the system itself (`generate_dataset.py`).

---

## ğŸ“¦ Deployment & Tooling

- **Package Management**: Managed by `uv` (as seen in `uv.lock` and `pyproject.toml`) for 10x faster dependency resolution.
- **Containerization**: Fully Dockerized with a multi-stage `Dockerfile` and `docker-compose.yml` for local orchestration of the API, UI, and Database.
- **UI Layer (`philoagents_ui/`)**: Multiple Streamlit entry points for testing different interaction paradigms (Streaming vs. Batch).

---

## ğŸ’¡ Why this architecture matters for an interview

- **Scalability**: The separation of application and infrastructure means you can switch from MongoDB to Pinecone, or Groq to OpenAI, by only changing the infrastructure layer.
- **Reliability**: By including an `evaluation/` module, you demonstrate that you build agents that can be tested and validated before deployment.
- **Efficiency**: Using `uv` and LangGraph shows you are utilizing the most performant and modern tools in the Python/AI ecosystem.
